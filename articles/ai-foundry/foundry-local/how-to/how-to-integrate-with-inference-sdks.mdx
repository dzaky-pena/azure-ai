---
title: "Integrate with inference SDKs"
description: This article provides instructions on how to integrate Foundry Local with common Inferencing SDKs.
ms.subservice: foundry-local
---

# Integrate inferencing SDKs with Foundry Local

import FoundryLocalPreview from '/snippets/ai-foundry/foundry-local/includes/foundry-local-preview.mdx'

<FoundryLocalPreview />

Foundry Local integrates with various inferencing SDKs - such as OpenAI, Azure OpenAI, Langchain, etc. This guide shows you how to connect your applications to locally running AI models using popular SDKs.

## Prerequisites

- Foundry Local installed. See the [Get started with Foundry Local](../get-started) article for installation instructions.

<Tabs>
<Tab title="Python">
import Python from '/snippets/ai-foundry/foundry-local/includes/integrate-examples/python.mdx'

<Python />
</Tab>
<Tab title="JavaScript">
import Javascript from '/snippets/ai-foundry/foundry-local/includes/integrate-examples/javascript.mdx'

<Javascript />
</Tab>
<Tab title="C#">
import CSharp from '/snippets/ai-foundry/foundry-local/includes/integrate-examples/csharp.mdx'

<CSharp />
</Tab>
<Tab title="Rust">
import Rust from '/snippets/ai-foundry/foundry-local/includes/integrate-examples/rust.mdx'

<Rust />
</Tab>
</Tabs>
## Next steps

- [Compile Hugging Face models to run on Foundry Local](how-to-compile-hugging-face-models)
- [Explore the Foundry Local CLI reference](../reference/reference-cli)
