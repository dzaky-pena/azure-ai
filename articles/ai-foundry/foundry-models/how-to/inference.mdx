---
title: "How to use the Azure AI Foundry Models inference endpoints to consume models"
description: Learn how to use the Azure AI Foundry Models inference endpoint to consume models
---

# Use Foundry Models

Once you have [deployed a model in Azure AI Foundry](../../model-inference/how-to/create-model-deployments), you can consume its capabilities via Azure AI Foundry APIs. There are two different endpoints and APIs to use models in Azure AI Foundry Models.

## Models inference endpoint

The models inference endpoint (usually with the form `https://<resource-name>.services.ai.azure.com/models`) allows customers to use a single endpoint with the same authentication and schema to generate inference for the deployed models in the resource. This endpoint follows the [Azure AI Model Inference API](../../model-inference/reference/reference-model-inference-api) which all the models in Foundry Models support. It supports the following modalities:

* Text embeddings
* Image embeddings
* Chat completions

### Routing

The inference endpoint routes requests to a given deployment by matching the parameter `name` inside of the request to the name of the deployment. This means that *deployments work as an alias of a given model under certain configurations*. This flexibility allows you to deploy a given model multiple times in the service but under different configurations if needed.

<Frame>
  ![](/images/ai-foundry/foundry-models/media/endpoint/endpoint-routing.png)
</Frame>

For example, if you create a deployment named `Mistral-large`, then such deployment can be invoked as:

import CodeCreateChatClient from '/snippets/ai-foundry/foundry-models/includes/code-create-chat-client.mdx'

<CodeCreateChatClient />

import CodeCreateChatCompletion from '/snippets/ai-foundry/foundry-models/includes/code-create-chat-completion.mdx'

<CodeCreateChatCompletion />

<Tip>
Deployment routing isn't case sensitive.
</Tip>

## Azure OpenAI inference endpoint

Azure AI Foundry also support the Azure OpenAI API. This API exposes the full capabilities of OpenAI models and supports additional features like assistants, threads, files, and batch inference. Non-OpenAI models can also be used for compatible functionalities.

Azure OpenAI endpoints (usually with the form `https://<resource-name>.openai.azure.com`) work at the deployment level and they have their own URL that is associated with each of them. However, the same authentication mechanism can be used to consume them. Learn more in the reference page for [Azure OpenAI API](../../openai/reference)

<Frame>
  ![](/images/ai-foundry/foundry-models/media/endpoint/endpoint-openai.png)
</Frame>

Each deployment has a URL that is the concatenations of the **Azure OpenAI** base URL and the route `/deployments/<model-deployment-name>`.

import CodeCreateOpenaiClient from '/snippets/ai-foundry/foundry-models/includes/code-create-openai-client.mdx'

<CodeCreateOpenaiClient />

import CodeCreateOpenaiChatCompletion from '/snippets/ai-foundry/foundry-models/includes/code-create-openai-chat-completion.mdx'

<CodeCreateOpenaiChatCompletion />


## Next steps

* [Use embedding models](../../model-inference/how-to/use-embeddings)
* [Use chat completion models](../../model-inference/how-to/use-chat-completions)
